# ðŸ“‹ TO DO - IMPLEMENTAÃ‡ÃƒO DE AVALIAÃ‡ÃƒO AUTOMÃTICA DE ÃUDIO

## ðŸŽ¯ **OBJETIVO**
Implementar sistema de avaliaÃ§Ã£o automÃ¡tica de Ã¡udio transcrito e diarizado usando IA da OpenAI, seguindo critÃ©rios configurados nas carteiras. A avaliaÃ§Ã£o serÃ¡ feita localmente (sem salvar no banco inicialmente) e permitirÃ¡ ediÃ§Ã£o manual posterior.

---

## ðŸ—ï¸ **BACKEND - Estrutura de Dados**

### 1. **Modelos de Dados (Pydantic)**
- [ ] Criar `AvaliacaoCreate` e `AvaliacaoOut` models
- [ ] Criar `ItemAvaliadoCreate` e `ItemAvaliadoOut` models  
- [ ] Criar `AvaliacaoRequest` para receber dados da IA
- [ ] Criar `AvaliacaoResponse` para retornar resultados

### 2. **Tabelas do Banco de Dados**
- [X] Criar tabela `criterios` (id, nome, descricao, categoria, ativo) - **JÃ EXISTE**
- [X] Criar tabela `avaliacoes` (id, call_id, agent_id, data_ligacao, status_avaliacao, pontuacao, carteira, criado_em) - **JÃ EXISTE**
- [X] Criar tabela `itens_avaliados` (id, avaliacao_id, criterio_id, descricao, resultado, peso, observacao) - **JÃ EXISTE**
- [X] Atualizar `init_db.py` com as novas tabelas - **JÃ FEITO**
- [X] **NOTA**: CritÃ©rios sÃ£o dinÃ¢micos e podem ser adicionados sem prejuÃ­zo da avaliaÃ§Ã£o

---

## ðŸ¤– **BACKEND - IntegraÃ§Ã£o com IA**

### 3. **ServiÃ§o de AvaliaÃ§Ã£o IA**
- [ ] Criar `AvaliacaoIAService` class
- [ ] Implementar mÃ©todo `avaliar_ligacao()` baseado na funÃ§Ã£o fornecida
- [ ] Implementar mÃ©todo `redistribuir_pesos_e_pontuacao()` baseado na funÃ§Ã£o fornecida
- [ ] Configurar modelo OpenAI: **gpt-4.1-mini**
- [ ] Implementar prompt engineering para OpenAI
- [ ] Implementar parsing da resposta da IA (JSON)
- [ ] Implementar tratamento de erros e fallback
- [ ] Implementar cÃ¡lculo de pontuaÃ§Ã£o (0-100)
- [ ] Implementar redistribuiÃ§Ã£o de pesos para critÃ©rios "NÃƒO SE APLICA"

### 4. **Endpoint de AvaliaÃ§Ã£o**
- [X] Criar `POST /avaliacao/automatica` endpoint
- [X] Receber: transcriÃ§Ã£o, carteira_id, call_id, agent_id
- [X] Buscar critÃ©rios da carteira
- [X] Chamar serviÃ§o de IA (simulado por enquanto)
- [X] **NÃƒO salvar no banco inicialmente** (apenas retornar resultado)
- [X] Retornar resultado da avaliaÃ§Ã£o estruturado

### 5. **Endpoints de Consulta**
- [X] `GET /avaliacao/{avaliacao_id}` - Buscar avaliaÃ§Ã£o especÃ­fica (placeholder)
- [X] `GET /avaliacao/{avaliacao_id}/itens` - Buscar itens da avaliaÃ§Ã£o (placeholder)
- [X] `PUT /avaliacao/{avaliacao_id}/item/{criterio_id}` - Editar item especÃ­fico (pendente)
- [X] `GET /criterios/` - Listar todos os critÃ©rios
- [X] `GET /carteira/{carteira_id}/criterios` - Listar critÃ©rios da carteira

---

## ðŸŽ¨ **FRONTEND - Interface de AvaliaÃ§Ã£o**

### 6. **Novo Card na PÃ¡gina de TranscriÃ§Ã£o**
- [X] Adicionar novo card em `AudioUpload.tsx` para "AvaliaÃ§Ã£o AutomÃ¡tica"
- [X] Componente de seleÃ§Ã£o de carteira (usa mesma carteira do upload)
- [X] **AvaliaÃ§Ã£o automÃ¡tica apÃ³s transcriÃ§Ã£o** (sem botÃ£o manual)
- [X] Loading state durante processamento
- [X] ExibiÃ§Ã£o dos resultados da avaliaÃ§Ã£o
- [X] BotÃ£o para editar resultados manualmente

### 7. **Componente de Resultados**
- [X] Criar `AvaliacaoResultados.tsx` component
- [X] Exibir pontuaÃ§Ã£o geral (0-100)
- [X] Exibir status (APROVADA/REPROVADA) - **limite: 70%**
- [X] Lista de critÃ©rios com resultados
- [X] ObservaÃ§Ãµes da IA para cada critÃ©rio
- [X] Possibilidade de editar resultados manualmente
- [ ] BotÃ£o para salvar avaliaÃ§Ã£o no banco (quando implementado)

### 8. **IntegraÃ§Ã£o com PÃ¡ginas Existentes**
- [X] Atualizar `AudioUpload.tsx` para incluir card de avaliaÃ§Ã£o automÃ¡tica
- [ ] Atualizar `CallItems.tsx` para mostrar resultados da IA
- [ ] Atualizar `Transcription.tsx` para incluir avaliaÃ§Ã£o
- [ ] Adicionar navegaÃ§Ã£o entre transcriÃ§Ã£o e avaliaÃ§Ã£o

---

## ðŸ”— **FRONTEND - API Integration**

### 9. **FunÃ§Ãµes de API**
- [X] `avaliarTranscricaoAutomatica()` - Chamar avaliaÃ§Ã£o IA
- [X] `getAvaliacaoById()` - Buscar avaliaÃ§Ã£o (jÃ¡ existe)
- [X] `getItensAvaliacao()` - Buscar itens da avaliaÃ§Ã£o
- [X] `updateItemAvaliacao()` - Atualizar item (jÃ¡ existe)
- [X] `getCriterios()` - Listar critÃ©rios
- [X] `getCriteriosCarteira()` - Listar critÃ©rios da carteira

### 10. **Hooks e Estados**
- [X] `useAvaliacaoAutomatica()` - Hook para gerenciar avaliaÃ§Ã£o
- [X] `useCriterios()` - Hook para buscar critÃ©rios
- [X] Estados para loading, erro, sucesso
- [X] Cache de critÃ©rios por carteira

---

## ðŸŽ¯ **LÃ³gica de NegÃ³cio**

### 11. **CÃ¡lculo de PontuaÃ§Ã£o**
- [ ] Implementar lÃ³gica de redistribuiÃ§Ã£o de pesos baseada na funÃ§Ã£o fornecida
- [ ] CritÃ©rios "NÃƒO SE APLICA" = peso 0
- [ ] CritÃ©rios vÃ¡lidos = peso redistribuÃ­do igualmente
- [ ] PontuaÃ§Ã£o = (conformes / total_vÃ¡lidos) * 100
- [ ] **Status = APROVADA se >= 70, REPROVADA se < 70**

### 12. **Mapeamento de Resultados**
- [ ] `CONFORME` â†’ "CONFORME"
- [ ] `NÃƒO CONFORME` â†’ "NAO CONFORME"  
- [ ] `NÃƒO SE APLICA` â†’ "NAO SE APLICA"
- [ ] FunÃ§Ã£o `map_resultado_value()` (jÃ¡ existe no cÃ³digo fornecido)

### 13. **Tratamento de Falha CrÃ­tica**
- [ ] Implementar lÃ³gica especial para "Falha CrÃ­tica"
- [ ] Se "Falha CrÃ­tica" = "NÃ£o Conforme" â†’ pontuaÃ§Ã£o = 0%
- [ ] Caso contrÃ¡rio â†’ cÃ¡lculo normal

---

## ðŸ”§ **ConfiguraÃ§Ã£o e Deploy**

### 14. **VariÃ¡veis de Ambiente**
- [ ] `OPENAI_API_KEY` - Chave da API OpenAI
- [ ] `OPENAI_MODEL` - Modelo: **gpt-4.1-mini**
- [ ] `AVALIACAO_THRESHOLD` - Limite para aprovaÃ§Ã£o: **70**

### 15. **DependÃªncias**
- [ ] Adicionar `openai` no `requirements.txt`
- [ ] Configurar timeout adequado para chamadas da IA
- [ ] Implementar retry logic para falhas de API

---

## ðŸ“Š **Monitoramento e Logs**

### 16. **Logs e Debug**
- [ ] Logs detalhados do processo de avaliaÃ§Ã£o
- [ ] Logs de prompts enviados para IA
- [ ] Logs de respostas recebidas
- [ ] MÃ©tricas de tempo de processamento
- [ ] Tratamento de erros da API OpenAI

---

## ðŸ§ª **Testes e ValidaÃ§Ã£o**

### 17. **Testes**
- [ ] Testes unitÃ¡rios do serviÃ§o de IA
- [ ] Testes de integraÃ§Ã£o dos endpoints
- [ ] Testes de cÃ¡lculo de pontuaÃ§Ã£o
- [ ] Testes de redistribuiÃ§Ã£o de pesos
- [ ] ValidaÃ§Ã£o de prompts com diferentes cenÃ¡rios

---

## ðŸ“ **HistÃ³rico e Versionamento**

### 18. **Controle de VersÃ£o**
- [ ] Manter histÃ³rico de avaliaÃ§Ãµes automÃ¡ticas vs manuais
- [ ] Identificar origem da avaliaÃ§Ã£o (automÃ¡tica/manual)
- [ ] Permitir comparaÃ§Ã£o entre versÃµes
- [ ] Log de mudanÃ§as manuais em avaliaÃ§Ãµes automÃ¡ticas

---

## ðŸš€ **FASES DE IMPLEMENTAÃ‡ÃƒO**

### **FASE 1 - Estrutura Base**
1. Modelos de dados e tabelas
2. ServiÃ§o de IA bÃ¡sico
3. Endpoint de avaliaÃ§Ã£o automÃ¡tica
4. Card bÃ¡sico na pÃ¡gina de transcriÃ§Ã£o

### **FASE 2 - Interface Completa**
1. Componente de resultados
2. EdiÃ§Ã£o manual de itens
3. IntegraÃ§Ã£o com pÃ¡ginas existentes
4. Hooks e estados

### **FASE 3 - PersistÃªncia**
1. Salvar avaliaÃ§Ãµes no banco
2. HistÃ³rico de avaliaÃ§Ãµes
3. ComparaÃ§Ã£o automÃ¡tica vs manual
4. RelatÃ³rios e mÃ©tricas

---

## â“ **DECISÃ•ES TÃ‰CNICAS**

### **Confirmadas:**
- âœ… Modelo OpenAI: gpt-4.1-mini
- âœ… Limite de aprovaÃ§Ã£o: 70%
- âœ… CritÃ©rios vÃªm da tabela (jÃ¡ inseridos pelo usuÃ¡rio)
- âœ… Interface: novo card na pÃ¡gina de transcriÃ§Ã£o
- âœ… EdiÃ§Ã£o manual: permitida
- âœ… HistÃ³rico: mantido

### **Pendentes:**
- [ ] Estrutura do prompt para IA
- [ ] Formato JSON de resposta esperado
- [ ] CritÃ©rios padrÃ£o para teste
- [ ] Timeout para chamadas da API

---

## ðŸ“‹ **CHECKLIST DE VALIDAÃ‡ÃƒO**

### **Antes de Implementar:**
- [ ] Confirmar estrutura do prompt
- [ ] Testar API OpenAI com exemplo simples
- [ ] Validar formato JSON de resposta
- [ ] Definir critÃ©rios de teste

### **Durante ImplementaÃ§Ã£o:**
- [ ] Testar cada endpoint individualmente
- [ ] Validar cÃ¡lculos de pontuaÃ§Ã£o
- [ ] Testar redistribuiÃ§Ã£o de pesos
- [ ] Validar interface responsiva

### **ApÃ³s ImplementaÃ§Ã£o:**
- [ ] Teste end-to-end completo
- [ ] ValidaÃ§Ã£o com dados reais
- [ ] Performance e timeout
- [ ] Tratamento de erros

---

*Ãšltima atualizaÃ§Ã£o: 2024-12-19*
*VersÃ£o: 1.4* 